{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning w/Python 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Packages / Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages Overview\n",
    "* The Natural Language Toolkit (nltk) - we'll be accessing corpora as well as functional tools from this package.\n",
    "    * The Brown Corpus: A text corpus of American English, split into fifteen different categories\n",
    "    * Part of Speech Taggers (pos): prebuilt functions that are designed to determine the part of speech of every word in a given sentence.\n",
    "* pandas - for data processing    \n",
    "* matplotlib - for visualizing data (%matplotlib inline - displays images clearly in the Jupyter notebook)\n",
    "* scikit-learn - for machine learning (ft. various classification, regression and clustering algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag_sents\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in brown.categories():\n",
    "    print (cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sent = brown.sents(categories=[\"news\"])\n",
    "romance_sent = brown.sents(categories=[\"romance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (news_sent[:5])\n",
    "print ()\n",
    "print (romance_sent[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame({'sentence': news_sent,\n",
    "                    'label':'news'})\n",
    "rdf = pd.DataFrame({'sentence':romance_sent, \n",
    "                    'label':'romance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining two spreadsheets into 1\n",
    "df = pd.concat([ndf, rdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features from our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "\n",
    "Supervised machine learning takes places in two steps: the training phase, and the testing phase. In the training phase, you use a portion of your data to train your algorithm (which, in our case, is a classification algorithm). You provide both your feature vector and your labels to the algorithm, and the algorithm searches for patterns in your data that can help associate it with a particular label.\n",
    "\n",
    "In the testing phase, we use the classifier we trained in the previous step, and give it previously unseen feature vectors representing unseen data to the algorithm, and have the algorithm predict the label. We can then compare the \"true\" label to the predicted label, and see if our classifier provides us with a good and generalizable way of accomplishing the task (in our case, the task of automatically distinguishing news sentences from romance sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning\n",
    "\n",
    "In supervised machine learning tasks, the data is assigned to some set of classes. For example, here we are given a dataset wherein each observation is a set of physical attributes of an object. In an supervised task, the object column acts as the labels. The algorithm then uses these existing separations in the data to develop criteria for classifying unknown observations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirchlet Allocation (LDA)\n",
    "\n",
    "One subset of unsupervised learning tasks are topic extraction tasks, where the aim is to find common groupings of items across collections of items. One method of doing so is Latent Dirichlet allocation (LDA). Latent Dirichlet Allocation is a way to model how topics are distributed over a corpus and words are distributed over a set of topics.\n",
    "\n",
    "In broad strokes, LDA extracts hidden (latent) topics via the following steps:1, 2\n",
    "\n",
    "1. Arbitrarily decide that there are 10 topics.\n",
    "2. Select one document and randomly assign each word in the document to one of the 10 topics.\n",
    "3. Repeat step 2 for all the other documents. This results in the same word being assigned to multiple topics.\n",
    "4. Compute\n",
    "    * how many topics are in each document?\n",
    "    * how many topic assignements are due to a given word?\n",
    "5. Take one word in one document and reassign it to a new topic and then repeat step 4.\n",
    "6. Repeat step 5 until the model stabilizes such that reassigned topics do not change distributions.\n",
    "\n",
    "LDA yields a set of words associated to each topic (see step 4, part 2) and the mixture of topics associated to each document (see step 4, part 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do topic modeling with sklearn!\n",
    "\n",
    "One of the best things about sklearn is the simplicity of its syntax.\n",
    "\n",
    "To do machine learning with sklearn, follow these five steps (the function names remain the same, regardless of the algorithm you use!):\n",
    "\n",
    "### Step 1: Import your desired algorithm\n",
    "\n",
    "In this example, we will be using the Latent Dirichlet Allocation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose your machine learning algorithm\n",
    "\n",
    "When creating an instance of sklearn's LatentDirichletAllocation algorithm to run on our data, we need to set parameters. n_components is the number of topics in the dataset and we set random_state to 42 so that this notebook is reproducible. Since the sentences happen to already have labels (either news or romance), lets see if LDA can also find those separations by setting the number of topics to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit your data\n",
    "\n",
    "Using the lda object we set up above, we now apply (fit) the LDA algorithm to the bag of words we extracted from our sentences and had stored in the tf sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=2, n_jobs=1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=42,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Transform your data\n",
    "\n",
    "We now want to model the documents in our corpus in terms of the topics discovered by the model. This is done using the .transform method of LDA. This function yields the distribution of topics across the documents. The document_topic array contains the percentages of each topic found in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize how much of each document is each topic - for example that document 1 is 10% topic A and 25% topic b. We choose an area chart because each band of the chart maps to a different category (in this case a unique topic). The width of each band in relation to the others illustrates how much of the document is thought to be about that topic relative to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import numpy as np\n",
    "\n",
    "colors = ['tab:green', 'tab:pink']\n",
    "topics = np.arange(10)\n",
    "num_docs = document_topic.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "_ = ax.stackplot(range(num_docs), document_topic.T, labels=topics, colors=colors)\n",
    "_ = ax.set_xlim(0, num_docs)\n",
    "_ = ax.set_ylim(0,1)\n",
    "_ = ax.set_yticks([])\n",
    "_ = ax.set_xlabel(\"document\")\n",
    "_ = ax.legend(title=\"topic\", bbox_to_anchor=(1.06, 1), borderaxespad=0)\n",
    "fig.savefig(\"images/doc_topic.png\", bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Print topics\n",
    "\n",
    "lda.components_ is an array where each row is a topic, and each column roughly contains the number of times that word was assigned to that topic, which is also the probability of that word being in that topic. To figure out which word is in which column, we use the get_feature_names() function from CountVectorizer. The argsort function is used to return the indexes of the columns with the highest probabilities, which we then map into our collection of words. Here we print the top 5 words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10\n",
    "topic_word  = lda.components_ \n",
    "words = np.array(tf_vectorizer.get_feature_names())\n",
    "for i, topic in enumerate(topic_word):\n",
    "    # sorting is in descending, so ::-1 reverses to ascending\n",
    "    sorted_idx = topic.argsort()[::-1]\n",
    "    print(i, words[sorted_idx][:num_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 ['said' 'like' 'time' 'just' 'll' 'way' 'didn' 'new' 'president' 'thought']\n",
    "1 ['mrs' 'said' 'home' 'little' 'year' 'day' 'good' 'new' 'got' 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize these topics as lists sized by the frequency of the word and colored by the topic, as proposed by Allan Riddell in Text Analysis with Topic Models for the Humanities and Social Sciences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# font size for word with largest share in corpus\n",
    "fontsize_base = 40/ np.max(topic_word)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 2), constrained_layout=True)\n",
    "\n",
    "for i, topic in enumerate(topic_word):\n",
    "    top_idx = topic.argsort()[::-1][:num_words]\n",
    "    top_words = words[top_idx]\n",
    "    top_share = topic[top_idx]\n",
    "    for j, (word, share) in enumerate(zip(top_words, top_share)):\n",
    "        ax.text(j, i/4,  word, fontsize=fontsize_base*share, color=colors[i])\n",
    "        \n",
    "#stretch the-axis to accommodate the words\n",
    "ax.set_xlim(0, num_words)\n",
    "ax.set_ylim(-.2, i/4+.2)\n",
    "ax.axis('off')\n",
    "#fig.subplots_adjust(hspace=-0)\n",
    "fig.savefig(\"images/word_topic.png\", bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "At the end of this workshop, we have covered the following skills:\n",
    "* How to use skills from the NLTK workshop to build features for a classification task\n",
    "* How to build a text classification system that can predict whether sentences belong to one category (\"news\") or another (\"romance\")\n",
    "* How to group data and perform calculations on the aggregations\n",
    "* How to prepare data for machine learning using pandas, a package for Python that helps to organize your data\n",
    "* How to use the scikit-learn package for Python to perform different types of machine learning on the data\n",
    "* How to evaluate the results of machine learning algorithms\n",
    "* How to visualize observations, aggregations, and algorithmic results\n",
    "\n",
    "## Resources\n",
    "\"Introduction to Machine Learning with Python\", Andreas C. Muller and Sarah Guido. O'Reilly, 2017.\n",
    "\n",
    "\"LING 83800: Methods in Computational Linguistics II\", Andrew Rosenberg. http://eniac.cs.qc.cuny.edu/andrew/methods2/, 2014.\n",
    "\n",
    "\"Introduction to Latent Dirichlet Allocation\", Edward Chen, http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/, 08/22/2011\n",
    "\n",
    "\"Topic Modeling for Humanists: A Guided Tour\", Scott Weingart, http://www.scottbot.net/HIAL/index.html@p=19113.html, 07/25/2012\n",
    "\n",
    "\"The LDA Buffet is Now Open\", Matthew Jockers, http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/, 09/29/2011\n",
    "\n",
    "\"Introduction to Topic Modeling\",Christine Doig, http://chdoig.github.io/pytexas2015-topic-modeling/#/, PyTexas, 2015\n",
    "\n",
    "##### Acknowledgments\n",
    "Shout out to the CUNY DHRI for providing much of this workshop's structure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
